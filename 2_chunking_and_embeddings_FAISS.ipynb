{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16281c7f",
   "metadata": {},
   "source": [
    "# CSR RAG Project - Step 2: Chunking and Vector Database (FAISS Version)\n",
    "# This notebook chunks the extracted text and creates embeddings using FAISS\n",
    "\n",
    "\n",
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383130be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antonius marcelino\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4e3d2",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904af895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Input file (from previous step)\n",
    "INPUT_FILE = \"extracted_csr_data.json\"\n",
    "\n",
    "# Chunking settings\n",
    "CHUNK_SIZE = 1000  # tokens (approximately)\n",
    "CHUNK_OVERLAP = 200  # tokens overlap between chunks\n",
    "\n",
    "# Vector database settings\n",
    "VECTOR_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# Embedding model (multilingual, free, local)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea1911",
   "metadata": {},
   "source": [
    "\n",
    "## Load Extracted Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c07864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extracted CSR data...\n",
      "‚úÖ Loaded 25 documents\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"Loading extracted CSR data...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    extracted_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(extracted_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048334f",
   "metadata": {},
   "source": [
    "\n",
    "## Initialize Text Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378a8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text splitter initialized\n",
      "   Chunk size: 1000 characters\n",
      "   Overlap: 200 characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Text splitter initialized\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"   Overlap: {CHUNK_OVERLAP} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78499e",
   "metadata": {},
   "source": [
    "\n",
    "## Create Chunks with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18534ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHUNKING DOCUMENTS\n",
      "============================================================\n",
      "\n",
      "üìÑ Danone 2019:\n",
      "   Original length: 75,916 characters\n",
      "   Created: 101 chunks\n",
      "\n",
      "üìÑ Danone 2020:\n",
      "   Original length: 75,916 characters\n",
      "   Created: 101 chunks\n",
      "\n",
      "üìÑ Danone 2021:\n",
      "   Original length: 75,717 characters\n",
      "   Created: 104 chunks\n",
      "\n",
      "üìÑ Danone 2022:\n",
      "   Original length: 75,717 characters\n",
      "   Created: 104 chunks\n",
      "\n",
      "üìÑ Danone 2024:\n",
      "   Original length: 135,522 characters\n",
      "   Created: 176 chunks\n",
      "\n",
      "üìÑ Indofood 2020:\n",
      "   Original length: 107,239 characters\n",
      "   Created: 137 chunks\n",
      "\n",
      "üìÑ Indofood 2021:\n",
      "   Original length: 215,448 characters\n",
      "   Created: 294 chunks\n",
      "\n",
      "üìÑ Indofood 2022:\n",
      "   Original length: 272,688 characters\n",
      "   Created: 358 chunks\n",
      "\n",
      "üìÑ Indofood 2023:\n",
      "   Original length: 334,431 characters\n",
      "   Created: 436 chunks\n",
      "\n",
      "üìÑ Indofood 2024:\n",
      "   Original length: 363,712 characters\n",
      "   Created: 480 chunks\n",
      "\n",
      "üìÑ Mayora 2019:\n",
      "   Original length: 13,627 characters\n",
      "   Created: 18 chunks\n",
      "\n",
      "üìÑ Mayora 2020:\n",
      "   Original length: 82,823 characters\n",
      "   Created: 115 chunks\n",
      "\n",
      "üìÑ Mayora 2021:\n",
      "   Original length: 103,243 characters\n",
      "   Created: 135 chunks\n",
      "\n",
      "üìÑ Mayora 2023:\n",
      "   Original length: 82,615 characters\n",
      "   Created: 108 chunks\n",
      "\n",
      "üìÑ Mayora 2024:\n",
      "   Original length: 114,180 characters\n",
      "   Created: 149 chunks\n",
      "\n",
      "üìÑ Ultra_jaya 2020:\n",
      "   Original length: 14,069 characters\n",
      "   Created: 18 chunks\n",
      "\n",
      "üìÑ Ultra_jaya 2021:\n",
      "   Original length: 14,156 characters\n",
      "   Created: 18 chunks\n",
      "\n",
      "üìÑ Ultra_jaya 2022:\n",
      "   Original length: 13,811 characters\n",
      "   Created: 17 chunks\n",
      "\n",
      "üìÑ Ultra_jaya 2023:\n",
      "   Original length: 13,896 characters\n",
      "   Created: 17 chunks\n",
      "\n",
      "üìÑ Ultra_jaya 2024:\n",
      "   Original length: 25,521 characters\n",
      "   Created: 33 chunks\n",
      "\n",
      "üìÑ Unilever 2020:\n",
      "   Original length: 254,034 characters\n",
      "   Created: 331 chunks\n",
      "\n",
      "üìÑ Unilever 2021:\n",
      "   Original length: 332,044 characters\n",
      "   Created: 428 chunks\n",
      "\n",
      "üìÑ Unilever 2022:\n",
      "   Original length: 362,785 characters\n",
      "   Created: 472 chunks\n",
      "\n",
      "üìÑ Unilever 2023:\n",
      "   Original length: 378,467 characters\n",
      "   Created: 498 chunks\n",
      "\n",
      "üìÑ Unilever 2024:\n",
      "   Original length: 388,322 characters\n",
      "   Created: 507 chunks\n"
     ]
    }
   ],
   "source": [
    "def create_chunks(extracted_data):\n",
    "    \"\"\"\n",
    "    Convert extracted data to LangChain documents with chunks\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    stats = {\n",
    "        \"total_documents\": len(extracted_data),\n",
    "        \"total_chunks\": 0,\n",
    "        \"chunks_by_company\": {}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CHUNKING DOCUMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for doc_data in extracted_data:\n",
    "        company = doc_data[\"company\"]\n",
    "        year = doc_data[\"year\"]\n",
    "        text = doc_data[\"text\"]\n",
    "        \n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        print(f\"\\nüìÑ {company} {year}:\")\n",
    "        print(f\"   Original length: {len(text):,} characters\")\n",
    "        print(f\"   Created: {len(chunks)} chunks\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadata = {\n",
    "                \"company\": company,\n",
    "                \"year\": year,\n",
    "                \"source_file\": doc_data[\"source_file\"],\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"page_count\": doc_data[\"page_count\"],\n",
    "                \"chunk_id\": f\"{company}_{year}_chunk_{i}\"\n",
    "            }\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            \n",
    "            all_chunks.append(doc)\n",
    "        \n",
    "        stats[\"total_chunks\"] += len(chunks)\n",
    "        if company not in stats[\"chunks_by_company\"]:\n",
    "            stats[\"chunks_by_company\"][company] = 0\n",
    "        stats[\"chunks_by_company\"][company] += len(chunks)\n",
    "    \n",
    "    return all_chunks, stats\n",
    "\n",
    "chunks, chunking_stats = create_chunks(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39eb20",
   "metadata": {},
   "source": [
    "\n",
    "## Display Chunking Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66087fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHUNKING STATISTICS\n",
      "============================================================\n",
      "Total documents: 25\n",
      "Total chunks created: 5155\n",
      "Average chunks per document: 206.2\n",
      "\n",
      "üìä Chunks by Company:\n",
      "   Danone: 586 chunks\n",
      "   Indofood: 1705 chunks\n",
      "   Mayora: 525 chunks\n",
      "   Ultra_jaya: 103 chunks\n",
      "   Unilever: 2236 chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHUNKING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total documents: {chunking_stats['total_documents']}\")\n",
    "print(f\"Total chunks created: {chunking_stats['total_chunks']}\")\n",
    "print(f\"Average chunks per document: {chunking_stats['total_chunks']/chunking_stats['total_documents']:.1f}\")\n",
    "\n",
    "print(\"\\nüìä Chunks by Company:\")\n",
    "for company, count in sorted(chunking_stats['chunks_by_company'].items()):\n",
    "    print(f\"   {company}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ab922",
   "metadata": {},
   "source": [
    "\n",
    "## Preview Sample Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbcf299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE CHUNKS\n",
      "============================================================\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Company: Danone\n",
      "Year: 2019\n",
      "Chunk ID: Danone_2019_chunk_0\n",
      "Length: 904 characters\n",
      "\n",
      "Content preview (first 200 chars):\n",
      "--- Page 1 --- Melestarikan Kebaikan Lingkungan Laporan Keberlanjutan 2020 PT Tirta Investama (Danone-AQUA) 28 --- Page 2 --- Komitmen Danone-AQUA terhadap pelestarian lingkungan tercermin dalam setia...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Company: Danone\n",
      "Year: 2019\n",
      "Chunk ID: Danone_2019_chunk_1\n",
      "Length: 790 characters\n",
      "\n",
      "Content preview (first 200 chars):\n",
      ". Beranjak dari pemahaman ini, maka kami menaruh perhatian besar terhadap pemantauan kinerja serta upaya Perusahaan dalam memitigasi dan mengatasi dampak lingkungan yang disebabkan oleh operasi kami. ...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Company: Danone\n",
      "Year: 2019\n",
      "Chunk ID: Danone_2019_chunk_2\n",
      "Length: 875 characters\n",
      "\n",
      "Content preview (first 200 chars):\n",
      ". Merupakan upaya Danone-AQUA dalam menciptakan siklus hidup baru untuk seluruh kemasan plastik yang ada di pasaran dengan mengoptimalkan pengumpulan sampah secara bertanggung jawab serta menuju kemas...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE CHUNKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Company: {chunk.metadata['company']}\")\n",
    "    print(f\"Year: {chunk.metadata['year']}\")\n",
    "    print(f\"Chunk ID: {chunk.metadata['chunk_id']}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"\\nContent preview (first 200 chars):\")\n",
    "    print(chunk.page_content[:200] + \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89f50e",
   "metadata": {},
   "source": [
    "\n",
    "## Initialize Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422bcc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING EMBEDDING MODEL\n",
      "============================================================\n",
      "This may take a few minutes on first run...\n",
      "The model will be downloaded and cached locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antonius marcelino\\AppData\\Local\\Temp\\ipykernel_21312\\1966036690.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a31de1e596480f9deb27a558ca9eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antonius marcelino\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\antonius marcelino\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b02fdbf9e04c548c10959172e88712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a109705f440f497a8bbb57e37ab65ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d8d579bf5743ceae3e60c9190fbd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1953ac294abf4ef2844a49bc9681c641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43755e8e6db449d940bc1c8fcf4a4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f50ca48334b05bcd02aca404c6050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac4183e2102404483ece38cdba2deff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decabe1bd96345be94aab747b8fd54a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19553178e45346bc9aaa7625807463a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114d959d1e224b2abdc4c14c57630b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded!\n",
      "   Model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "   Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING EMBEDDING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "print(\"The model will be downloaded and cached locally.\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "print(f\"   Model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "print(f\"   Embedding dimension: {len(test_embedding)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1e65e",
   "metadata": {},
   "source": [
    "\n",
    "## Create FAISS Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ba7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING FAISS VECTOR DATABASE\n",
      "============================================================\n",
      "This will take several minutes...\n",
      "Creating embeddings for 5155 chunks...\n",
      "\n",
      "\n",
      "‚úÖ Vector database created successfully!\n",
      "   Total vectors: 5155\n",
      "   Saved to: faiss_index\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CREATING FAISS VECTOR DATABASE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will take several minutes...\")\n",
    "print(f\"Creating embeddings for {len(chunks)} chunks...\\n\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Vector database created successfully!\")\n",
    "print(f\"   Total vectors: {len(chunks)}\")\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.save_local(VECTOR_DB_PATH)\n",
    "print(f\"   Saved to: {VECTOR_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255948a",
   "metadata": {},
   "source": [
    "\n",
    "## Test Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81637852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING RETRIEVAL\n",
      "============================================================\n",
      "\n",
      "üîç Query: 'What is Unilever's water conservation program?'\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Company: Unilever\n",
      "  Year: 2024\n",
      "  Chunk: 86/507\n",
      "  Preview: . Sesuai komitmen keberlanjutan dalam GAP, Unilever secara global menargetkan penerapan program pengelolaan air (water stewardship) di 100 area rawan ...\n",
      "\n",
      "Result 2:\n",
      "  Company: Unilever\n",
      "  Year: 2022\n",
      "  Chunk: 168/472\n",
      "  Preview: . Globally, Unilever is part of Water Resources Group (WRG) 2030, working towards promoting water management resilience and transformative change in c...\n",
      "\n",
      "Result 3:\n",
      "  Company: Unilever\n",
      "  Year: 2020\n",
      "  Chunk: 301/331\n",
      "  Preview: . Unilever mengambil langkah nyata dalam menjaga ketersedian air melalui efisiensi penggunaan air dan pengurangan pencemaran terhadap air. Dalam efisi...\n",
      "\n",
      "üîç Query: 'Apa program CSR Indofood?'\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Company: Indofood\n",
      "  Year: 2020\n",
      "  Chunk: 135/137\n",
      "  Preview: . Seluruh kemasan Indofood terbuat dari bahan baku berkualitas untuk menjamin keamanan pangan. Selama tahun 2020, kami terus mendorong inovasi di bida...\n",
      "\n",
      "Result 2:\n",
      "  Company: Indofood\n",
      "  Year: 2023\n",
      "  Chunk: 373/436\n",
      "  Preview: . Indofood supports the development of specialized modules on nutrition and health, designed for health workers and Posyandu cadres that accessible at...\n",
      "\n",
      "Result 3:\n",
      "  Company: Indofood\n",
      "  Year: 2023\n",
      "  Chunk: 372/436\n",
      "  Preview: . Indofood has supported the development of specialized modules on nutrition and health. Platform Edukasi untuk Tenaga Kesehatan dan Kader Posyandu Tu...\n",
      "\n",
      "üîç Query: 'energy efficiency initiatives'\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Company: Danone\n",
      "  Year: 2024\n",
      "  Chunk: 18/176\n",
      "  Preview: . Efisiensi Energi Pengurangan intensitas energi sebesar 2,4%. Energi Terbarukan >50% penggunaan listrik kami berasal dari sumber energi terbarukan. M...\n",
      "\n",
      "Result 2:\n",
      "  Company: Unilever\n",
      "  Year: 2024\n",
      "  Chunk: 140/507\n",
      "  Preview: . [305-6] Efisiensi Energi [OJK F.7] Efisiensi energi merupakan bagian dari strategi Perseroan untuk mencapai target NZE pada tahun 2039. Kami menerap...\n",
      "\n",
      "Result 3:\n",
      "  Company: Unilever\n",
      "  Year: 2021\n",
      "  Chunk: 29/428\n",
      "  Preview: . Berikut beberapa upaya kami dalam upaya efisiensi energi: [F.7] ‚Ä¢ Melalui proyek ZARA - World Class Manufacturing, Unilever melakukan efisiensi ener...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is Unilever's water conservation program?\",\n",
    "    \"Apa program CSR Indofood?\",\n",
    "    \"energy efficiency initiatives\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Company: {doc.metadata['company']}\")\n",
    "        print(f\"  Year: {doc.metadata['year']}\")\n",
    "        print(f\"  Chunk: {doc.metadata['chunk_index']+1}/{doc.metadata['total_chunks']}\")\n",
    "        print(f\"  Preview: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78948b1",
   "metadata": {},
   "source": [
    "\n",
    "## Verify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7aa076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATABASE VERIFICATION\n",
      "============================================================\n",
      "‚úÖ Database verified!\n",
      "   Successfully loaded from disk\n",
      "   Database size: 20.13 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATABASE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test loading from disk\n",
    "test_vectorstore = FAISS.load_local(\n",
    "    VECTOR_DB_PATH, \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Database verified!\")\n",
    "print(f\"   Successfully loaded from disk\")\n",
    "\n",
    "# Calculate approximate size\n",
    "import os\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(VECTOR_DB_PATH, f))\n",
    "    for f in os.listdir(VECTOR_DB_PATH)\n",
    "    if os.path.isfile(os.path.join(VECTOR_DB_PATH, f))\n",
    ")\n",
    "print(f\"   Database size: {total_size/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fe0e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Save Metadata Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2954e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Summary saved to: vector_db_summary.json\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "    \"created_at\": chunking_stats,\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"total_chunks\": len(chunks),\n",
    "    \"companies\": list(chunking_stats['chunks_by_company'].keys()),\n",
    "    \"years\": [2019, 2020, 2021, 2022, 2023],\n",
    "    \"vector_db_type\": \"FAISS\"\n",
    "}\n",
    "\n",
    "with open(\"vector_db_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Summary saved to: vector_db_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa1dda",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "becd37ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ STEP 2 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Next: Open 03_rag_chatbot_FAISS.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ STEP 2 COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNext: Open 03_rag_chatbot_FAISS.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
